{"session_id":"1693627290","model":{"name":"gpt-4","endpoint":"https://api.openai.com/v1/completions","token_limit":4096},"interactions":[{"request":[{"role":"user","content":"testign"}],"response":{"id":"chatcmpl-7uC7BbeWw7f6xGzMy2h316CQKv2DD","object":"chat.completion","created":1693627293,"model":"gpt-4-0613","usage":{"prompt_tokens":9,"completion_tokens":9,"total_tokens":18},"choices":[{"index":0,"message":{"role":"assistant","content":"Hello! How can I assist you today?","function_call":null},"finish_reason":"stop"}]}},{"request":[{"role":"user","content":"just chillin"}],"response":{"id":"chatcmpl-7uC7H11VoLLS4JNzqDf8047kVgyrA","object":"chat.completion","created":1693627299,"model":"gpt-4-0613","usage":{"prompt_tokens":16,"completion_tokens":23,"total_tokens":39},"choices":[{"index":0,"message":{"role":"assistant","content":"I'm an AI developed by OpenAI and I'm ready to assist. What can I help you with today?","function_call":null},"finish_reason":"stop"}]}},{"request":[{"role":"user","content":"use async_openai::types::{Role, CreateChatCompletionRequest};\nuse async_openai::types::{ChatCompletionRequestMessage, CreateChatCompletionResponse};\nuse serde::{Deserialize, Serialize};\nuse std::fs;\nuse std::io;\nuse std::path::{Path, PathBuf};\nuse crate::errors::SessionManagerError;\nuse crate::gpt_connector::{GPTConnector, GPTSettings};\nuse crate::gpt_connector::Model;\nuse crate::utils;\n\npub const SESSIONS_DIR: &str = \"data/sessions\";\npub const INGESTED_DIR: &str = \"data/ingested\";\n\n#[derive(Debug, Serialize, Deserialize)]\n#[derive(Clone)]\npub struct ChatInteraction {\n    pub request: Vec<ChatCompletionRequestMessage>,\n    pub response: CreateChatCompletionResponse,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\n#[derive(Clone)]\npub struct Session {\n    pub session_id: String,\n    pub model: Model,\n    pub interactions: Vec<ChatInteraction>,\n}\n\nimpl Session {\n    pub fn new(session_id: String, model: Model) -> Self {\n        Self {\n            session_id,\n            model,\n            interactions: Vec::new(),\n        }\n    }\n}\n#[derive(Debug, Serialize, Deserialize)]\npub struct IngestedData {\n    session_id: String,\n    file_path: String,\n    chunk_num: u32,\n    content: String,\n}\npub struct SessionManager {\n    gpt_connector: GPTConnector,\n    cached_request: Option<Vec<ChatCompletionRequestMessage>>,\n    pub session_data: Session,\n}\n\nimpl SessionManager {\n    pub async fn new(settings: GPTSettings, session_data: Option<Session>) -> SessionManager {\n        let gpt_connector = GPTConnector::new(&settings).await;\n        let model = gpt_connector.model.clone();\n        match session_data {\n            Some(session_data) => Self {\n                gpt_connector,\n                cached_request: None,\n                session_data,\n            },\n            None => Self {\n                gpt_connector,\n                cached_request: None,\n                session_data: Session::new(utils::generate_session_id(), model),\n            }\n        }\n    }\n\n    pub fn save_session(&self) -> io::Result<()> {\n        Self::ensure_directory_exists(SESSIONS_DIR).unwrap();\n        let session_file_path = self.get_session_filepath();\n        let data = serde_json::to_string(&self.session_data)?;\n        fs::write(session_file_path, data)?;\n        self.save_last_session_file_path();\n        Ok(())\n    }\n\n\n\n    // Get the responses from the session data\n    pub fn get_responses(&self) -> Vec<CreateChatCompletionResponse> {\n        self.session_data\n            .interactions\n            .iter()\n            .map(|interaction| interaction.response.clone())\n            .collect()\n    }\n\n    // Get the chat history from the session data\n    pub fn get_chat_history(&self) -> Vec<(Role, String)> {\n        let mut chat_history: Vec<(Role, String)> = Vec::new();\n        for interaction in &self.session_data.interactions {\n            for request in self.get_request_messages() {\n                chat_history.push((request.role.clone(), request.content.clone().unwrap_or_default()));\n            }\n            for choice in &interaction.response.choices {\n                chat_history.push((choice.message.role.clone(), choice.message.content.clone().unwrap_or_default()));\n            }\n        }\n        chat_history\n    }\n    // Add an interaction to the session data\n    pub fn add_interaction(\n        &mut self,\n        request: Vec<ChatCompletionRequestMessage>,\n        response: CreateChatCompletionResponse,\n    ) {\n        self.session_data\n            .interactions\n            .push(ChatInteraction { request, response })\n    }\n\n    pub fn add_interaction_for_cached_request(\n        &mut self,\n        response: CreateChatCompletionResponse,\n    ) {\n        if let Some(request) = self.cached_request.clone() {\n            self.add_interaction(request, response);\n            self.cached_request = None;\n        }\n    }\n\n    pub fn get_last_session_file_path() -> Option<PathBuf> {\n        Self::ensure_directory_exists(SESSIONS_DIR).unwrap();\n        let last_session_path = Path::new(SESSIONS_DIR).join(\"last_session.txt\");\n        if last_session_path.exists() {\n            Some(fs::read_to_string(last_session_path).unwrap().into())\n        } else {\n            None\n        }\n    }\n\n    fn get_request_messages(&self) -> Vec<ChatCompletionRequestMessage> {\n        self.session_data\n            .interactions\n            .iter()\n            .map(|interaction| interaction.request.clone())\n            .flatten()\n            .collect()\n    }\n    pub async fn send_request(&mut self, request: CreateChatCompletionRequest) -> Result<CreateChatCompletionResponse, SessionManagerError> {\n        let response = self.gpt_connector.send_request(request.clone()).await?;\n        // println response\n        println!(\"response: {:?}\", response);\n        self.add_interaction_for_cached_request(response.clone());\n        Ok(response)\n    }\n    pub fn construct_request(&mut self, content: Vec<String> ) -> CreateChatCompletionRequest {\n        // iterate through the vector of ChatCompletionRequestMessage from the interactions stored in session_data as a clone\n        let mut messages = self.get_request_messages();\n\n        let mut new_messages: Vec<ChatCompletionRequestMessage> = Vec::new();\n        for item in content {\n            let message = ChatCompletionRequestMessage {\n                role: Role::User,\n                content: Some(item),\n                ..Default::default()\n            };\n            messages.push(message.clone());\n            new_messages.push(message);\n        }\n        self.cached_request = Some(new_messages);\n        // return a new CreateChatCompletionRequest\n        CreateChatCompletionRequest {\n            model: self.gpt_connector.model.name.clone(),\n            messages,\n            ..Default::default()\n        }\n    }\n\n    pub fn save_last_session_file_path(&self) {\n        Self::ensure_directory_exists(SESSIONS_DIR).unwrap();\n        let last_session_path = Path::new(SESSIONS_DIR).join(\"last_session.txt\");\n        fs::write(\n            last_session_path,\n            self.get_session_filepath().display().to_string(),\n        )\n        .unwrap();\n    }\n\n    fn ensure_directory_exists(dir: &str) -> io::Result<()> {\n        let dir_path = Path::new(dir);\n        if !dir_path.exists() {\n            fs::create_dir_all(&dir_path)?;\n        }\n        Ok(())\n    }\n\n    pub fn get_session_filepath(&self) -> PathBuf {\n        Path::new(SESSIONS_DIR).join(self.get_session_filename())\n    }\n\n    pub fn get_session_filename(&self) -> String {\n        format!(\"{}.json\", self.session_data.session_id)\n    }\n\n    pub fn get_ingested_filepath(&self) -> PathBuf {\n        Path::new(INGESTED_DIR).join(format!(\"{}.json\", self.session_data.session_id))\n    }\n\n    pub fn save_ingested_file(&self, content: &str) -> io::Result<()> {\n        Self::ensure_directory_exists(INGESTED_DIR)?;\n\n        let ingested_file_path = self.get_ingested_filepath();\n        fs::write(ingested_file_path, content)?;\n        Ok(())\n    }\n\n    /// This function takes in an input which could be a path to a directory, a path to a file,\n    /// a block of text, or a URL. Depending on the type of input, it processes (or ingests) the\n    /// content by converting it into chunks of text and then sends each chunk to the GPT API.\n    pub async fn handle_ingest(&mut self, chunks: Vec<String>) -> Result<(), SessionManagerError> {\n        let request = self.construct_request(chunks);\n        // Send each chunk to the GPT API using the GPTConnector.\n        let response = self.gpt_connector.send_request(request.clone()).await?;\n        // After successful ingestion, copy the file to the 'ingested' directory.\n        Ok(        self.add_interaction_for_cached_request(response.clone()))\n    }\n}\n\n// Tests\n#[cfg(test)]\nmod tests {\n\n    \n}"}],"response":{"id":"chatcmpl-7uC7dBdWV3cv83TxUN7sVPtutk88n","object":"chat.completion","created":1693627321,"model":"gpt-4-0613","usage":{"prompt_tokens":1676,"completion_tokens":213,"total_tokens":1889},"choices":[{"index":0,"message":{"role":"assistant","content":"This looks like Rust code that creates a cellular layer managing sessions for interactions with OpenAI's GPT3. Here's a brief rundown of the important parts:\n\n- `ChatInteraction`, `Session`, `IngestedData`, and `SessionManager` are structs that represent different concepts in the session management system.\n- `SessionManager` is the core of the session management code. It offers several methods to manipulate and retrieve information about a session.\n- The `SessionManager` struct contains a `GPTConnector` which seems to handle the communication between the application and the GPT3 API.\n- `handle_ingest` method accepts a `Vec<String>`, constructs a request and sends it to the GPT API. It's not clear from the provided code what specific purpose this serves.\n- Various methods handle saving and loading sessions, handling request-response interactions, and ingesting data.\n  \nPlease note that without the fully contextual code (like missing modules, missing methods in the same module, etc.), some of the specific functionalities might not be accurately described.","function_call":null},"finish_reason":"stop"}]}},{"request":[{"role":"user","content":""}],"response":{"id":"chatcmpl-7uC7vxGErBQnrA9OCAzWGAk8wHpTK","object":"chat.completion","created":1693627339,"model":"gpt-4-0613","usage":{"prompt_tokens":1680,"completion_tokens":13,"total_tokens":1693},"choices":[{"index":0,"message":{"role":"assistant","content":"You didn't ask anything. Can you please provide more information?","function_call":null},"finish_reason":"stop"}]}},{"request":[{"role":"user","content":"![Project Banner or Logo](docs/sazid_banner_image.png) # **Sazid: Ascended Rust Development** Sazid is an interactive chat application that connects users with OpenAI's GPT model. With the power of GPT and a user-friendly command-line interface, it allows seamless conversations and can process and import various file types. ## **Table of Contents** - [Features](#features) - [Getting Started](#getting-started) - [Prerequisites](#prerequisites) - [Installation](#installation) - [Usage](#usage) - [Contributing](#contributing) - [License](#license) - [Acknowledgements](#acknowledgements) ## **Features** - **GPT Integration**: Seamlessly connect with OpenAI's GPT model for interactive chats. - **Chat Session Management**: Start new sessions or continue previous ones with ease. - **File Ingestion**: Import and process a variety of file formats. - **Interactive CLI**: Engage with a user-friendly command-line interface. - **Error Handling**: Robust mechanisms to handle potential issues, ensuring smooth user experiences. ## **Getting Started** ### **Prerequisites** Before you start, ensure you have the following installed: - [Rust](https://www.rust-lang.org/) - [Cargo](https://doc.rust-lang.org/cargo/) You'll also need API keys for OpenAI's GPT. Store them securely as environment variables. ### **Installation** 1. Clone the repository: ```bash git clone https://github.com/your_username/sazid.git cd sazid ``` 2. Build the project: ```bash cargo build --release ``` 3. Run the application: ```bash cargo run --release ``` ## **Usage** To start a new chat session: ```bash sazid -n ``` To continue a previous session: ```bash sazid -c path_to_session_file ``` To import a file or directory: ```bash sazid -i path_to_file_or_directory ``` For a complete list of options: ```bash sazid --help ``` ## **Contributing** We welcome contributions! Please read [CONTRIBUTING.md](path_to_contributing.md) for details on our code of conduct and submission process. ## **License** This project is licensed under the MIT License - see the [LICENSE.md](path_to_license.md) file for details. ## **Acknowledgements** - [OpenAI](https://www.openai.com/) for providing the GPT model. - [Rust community](https://www.rust-lang.org/) for their comprehensive documentation and support."}],"response":{"id":"chatcmpl-7uCJZvIgXpPxRWDG5usIvgdTX5nUy","object":"chat.completion","created":1693628061,"model":"gpt-4-0613","usage":{"prompt_tokens":2223,"completion_tokens":0,"total_tokens":2223},"choices":[{"index":0,"message":{"role":"assistant","content":"","function_call":null},"finish_reason":"stop"}]}},{"request":[{"role":"user","content":""}],"response":{"id":"chatcmpl-7uCJjB88Gw8jKJXhtZ3sMjtEaECCR","object":"chat.completion","created":1693628071,"model":"gpt-4-0613","usage":{"prompt_tokens":2227,"completion_tokens":9,"total_tokens":2236},"choices":[{"index":0,"message":{"role":"assistant","content":"Hello! How can I assist you today?","function_call":null},"finish_reason":"stop"}]}}]}